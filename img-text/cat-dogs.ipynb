{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.0-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.54.1-cp312-cp312-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\alarc\\documents\\github\\ia\\venv\\lib\\site-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alarc\\documents\\github\\ia\\venv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alarc\\documents\\github\\ia\\venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\alarc\\documents\\github\\ia\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alarc\\documents\\github\\ia\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "Using cached contourpy-1.3.0-cp312-cp312-win_amd64.whl (218 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.54.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 kiwisolver-1.4.7 matplotlib-3.9.2 pyparsing-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n",
      "Found 4998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Crear generadores de datos para entrenamiento y validación\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255, validation_split=0.2)  # Escalar los valores de los píxeles\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './PetImages',                 # Directorio con los datos\n",
    "    target_size=(128, 128),         # Redimensionar imágenes a 128x128 píxeles\n",
    "    batch_size=32,\n",
    "    class_mode='binary',     \n",
    "    classes=['Cat', 'Dog'],        # Clasificación binaria (gato vs perro)\n",
    "    subset='training'               # Usar el 80% de las imágenes para entrenamiento\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    './PetImages',\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    classes=['Cat', 'Dog'], \n",
    "    subset='validation'             # Usar el 20% de las imágenes para validación\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en archivo: ./PetImages\\Cat\\666.jpg - cannot identify image file 'C:\\\\Users\\\\alarc\\\\Documents\\\\github\\\\IA\\\\img-text\\\\PetImages\\\\Cat\\\\666.jpg'\n",
      "Error en archivo: ./PetImages\\Dog\\11702.jpg - cannot identify image file 'C:\\\\Users\\\\alarc\\\\Documents\\\\github\\\\IA\\\\img-text\\\\PetImages\\\\Dog\\\\11702.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alarc\\Documents\\github\\IA\\venv\\Lib\\site-packages\\PIL\\TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Ruta de tu directorio de imágenes\n",
    "data_dir = './PetImages'\n",
    "\n",
    "# Verificar cada archivo en las carpetas de 'Cat' y 'Dog'\n",
    "for class_name in ['Cat', 'Dog']:\n",
    "    folder_path = os.path.join(data_dir, class_name)\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            # Intentar abrir la imagen\n",
    "            img = Image.open(file_path)\n",
    "            img.verify()  # Verificar si la imagen está corrupta\n",
    "            img.close()   # Cerrar la imagen para liberar memoria\n",
    "        except (IOError, SyntaxError, Image.DecompressionBombError) as e:\n",
    "            print(f\"Error en archivo: {file_path} - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 193ms/step - accuracy: 0.5821 - loss: 0.7005 - val_accuracy: 0.7185 - val_loss: 0.5500\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 196ms/step - accuracy: 0.7355 - loss: 0.5312 - val_accuracy: 0.7639 - val_loss: 0.4902\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 196ms/step - accuracy: 0.7983 - loss: 0.4345 - val_accuracy: 0.8219 - val_loss: 0.3981\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 205ms/step - accuracy: 0.8462 - loss: 0.3511 - val_accuracy: 0.8427 - val_loss: 0.3748\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 198ms/step - accuracy: 0.8776 - loss: 0.2845 - val_accuracy: 0.8407 - val_loss: 0.3696\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 192ms/step - accuracy: 0.9135 - loss: 0.2115 - val_accuracy: 0.8415 - val_loss: 0.3703\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 192ms/step - accuracy: 0.9495 - loss: 0.1338 - val_accuracy: 0.8513 - val_loss: 0.4457\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 192ms/step - accuracy: 0.9686 - loss: 0.0820 - val_accuracy: 0.8401 - val_loss: 0.4802\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 189ms/step - accuracy: 0.9773 - loss: 0.0625 - val_accuracy: 0.8521 - val_loss: 0.5259\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 204ms/step - accuracy: 0.9823 - loss: 0.0510 - val_accuracy: 0.8439 - val_loss: 0.6032\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Definir el modelo\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Salida con una unidad y activación sigmoid para clasificación binaria\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - accuracy: 0.8348 - loss: 0.6184\n",
      "Loss: 0.6031917929649353, Accuracy: 0.8439375758171082\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "# Guardar el modelo\n",
    "model.save('model-cat-dog.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Es un gato\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Cargar una imagen y procesarla\n",
    "img_path = './dog2.jpg'  # Cambia por la ruta a tu imagen\n",
    "img = image.load_img(img_path, target_size=(128, 128))\n",
    "img_array = image.img_to_array(img) / 255.0  # Normalizar como en entrenamiento\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Agregar dimensión de lote\n",
    "\n",
    "# Hacer la predicción\n",
    "prediction = model.predict(img_array)\n",
    "if prediction[0] > 0.5:\n",
    "    print(\"Es un perro\")\n",
    "else:\n",
    "    print(\"Es un gato\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
